{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fragrantica perfume review clasifier (LSTM with stopword removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model Name\n",
    "MODEL_NAME = 'lstm_with_stopword_removed_09'\n",
    "\n",
    "# HyperParameters\n",
    "PAD_LEN = 200\n",
    "NUM_WORDS = 5000\n",
    "EMBEDDING = 50\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '../dataset/dataset_210626_215600.csv'\n",
    "data_exist = path.exists(data_path)\n",
    "\n",
    "if not data_exist:\n",
    "    url = 'https://kyuuuw-nlp-dataset.s3.ap-northeast-2.amazonaws.com/fragrantica/dataset_210626_215600.csv'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(data_path, 'w').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74779\n",
      "74779\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "\n",
    "X_data = data['stopwords_removed']\n",
    "y_data = data['label']\n",
    "\n",
    "print(len(X_data))\n",
    "print(len(y_data))\n",
    "\n",
    "\n",
    "##### 토큰화 및 인덱스 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['got', 'sample', 'today', 'year', 'old', 'daughter', 'thought', 'smelling', 'sprayed', 'card', 'rotten', 'fish', 'nearly', 'threw', 'immediately', 'however', 'later', 'evening', 'decided', 'give', 'fair', 'trial', 'sprayed', 'crook', 'elbows', 'rotting', 'fish', 'smell', 'time', 'got', 'definite', 'bit', 'funk', 'almost', 'urine', 'scent', 'minutes', 'blossomed', 'gorgeous', 'smooth', 'woody', 'ambery', 'clean', 'warm', 'jasmine', 'daughter', 'didnt', 'even', 'believe', 'told', 'fragrance', 'smelled', 'earlier', 'lol', 'immediately', 'commented', 'fresh', 'clean', 'vibe', 'agree', 'soapy', 'hint', 'powdery', 'way', 'obsessed', 'cant', 'stop', 'smelling', 'arm', 'glad', 'gave', 'try', 'wait', 'buy', 'full', 'bottle']\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_data)\n",
    "sequences = tokenizer.texts_to_sequences(X_data)\n",
    "\n",
    "print(X_data[0])\n",
    "print(len(sequences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 인덱스별 단어 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 빈도수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "등장 빈도가 2번 이하인 희귀 단어의 수: 44912\n",
      "단어 집합(vocabulary)에서 희귀 단어의 비율: 0.552307635549762\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.013189565348140857\n"
     ]
    }
   ],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(word_to_index) # 총 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도 수가 threshold 보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0\n",
    "\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    if value < threshold:\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print(f\"등장 빈도가 {threshold}번 이하인 희귀 단어의 수: {rare_cnt}\")\n",
    "print(f\"단어 집합(vocabulary)에서 희귀 단어의 비율: {rare_cnt / total_cnt}\" )\n",
    "print(f\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율: {rare_freq / total_freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 등장 횟수 상위 50000개의 word 만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41, 102, 175, 207, 142, 1698, 134, 83, 145, 1237, 2100, 4180, 874, 2645, 466, 105, 190, 474, 404, 111, 1313, 4301, 145, 3232, 4180, 5, 17, 41, 1604, 38, 77, 2819, 2, 184, 249, 355, 129, 1269, 120, 70, 155, 1698, 646, 27, 273, 587, 4, 45, 1272, 333, 466, 2065, 39, 120, 198, 330, 533, 276, 170, 36, 1699, 343, 421, 83, 575, 477, 324, 60, 532, 43, 185, 12], [15, 2041, 310, 3487, 642, 8, 1, 1432, 55, 1, 7, 806, 496, 161, 16, 8, 689, 3292, 57, 100, 67, 25, 161, 37, 801, 16, 168, 16, 343, 1263, 1095, 494, 7, 47, 90, 849, 12, 1388, 838, 404, 111, 60, 145, 3487, 273, 1762, 428, 215, 115, 1546, 57, 7, 155, 810, 35, 20, 350, 5, 5, 155, 1103, 70, 139, 442, 36, 20, 228, 572, 52, 11, 689, 459, 82, 1494, 73, 248, 632, 273, 67, 444, 138, 4, 405, 3487, 22, 8, 100, 19, 33, 244, 213, 101, 686, 2234, 16, 16, 252, 228, 1190, 1813, 22, 115, 46, 19, 1042, 12, 331, 6, 331, 187, 224, 621, 1686, 43, 12, 3753, 236, 17, 310, 268, 11, 1539], [7, 132, 58, 385, 133, 7, 132, 47, 26, 9, 454, 41, 102, 375, 1317, 510, 1185, 109, 1451, 85, 248, 234, 732, 1348, 601, 212, 4, 18, 155, 171, 416, 944, 212], [340, 985, 1954, 525, 15, 155, 185, 2531, 15, 29, 5, 823, 1783, 369, 279, 155, 459, 892, 53, 17, 1384, 154, 117, 212, 119, 155, 399, 624, 3701, 66, 132, 79, 14, 6, 1568, 1, 3, 19, 191, 875, 210, 1063, 49, 45, 76, 163, 1458, 995, 379, 38, 345, 164, 603, 2244, 647, 23, 661, 15, 55, 774, 226, 86, 86, 2720, 9, 2195, 50, 196, 88, 983, 6, 417, 1348, 133, 212, 18, 105, 171, 416, 320, 2235, 5, 20, 1, 212, 80], [249, 249, 397, 6, 2, 294, 2, 142, 181, 33, 169, 43, 1, 69, 65, 43]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X_data)\n",
    "sequences = tokenizer.texts_to_sequences(X_data)\n",
    "\n",
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56084\n",
      "56084\n",
      "18695\n",
      "18695\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences, y_data)\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1598, 81, 2011, 429, 155, 1088, 1579, 1708, 385, 556, 2372, 4, 1994, 12, 632, 1994, 293, 134, 2648, 3661, 136, 73, 44, 375, 83, 429, 2049, 35, 134, 260, 269, 41, 12, 440, 4, 56, 25, 3, 9, 1461, 609, 1088, 1683, 525, 2957, 1348, 601, 1464, 17, 1640, 28, 1181, 696, 548, 668, 1561, 1584, 172, 4293, 29, 3, 330, 394, 451, 88, 1732, 127, 28, 37, 3713, 1017, 942, 570, 32, 125, 4, 4860, 578, 225, 3, 195, 649, 500, 113, 401, 36, 880, 391, 56, 25, 3479, 968, 440, 1579, 2049, 2861, 69, 2694]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# padding and trimming\n",
    "X_train = pad_sequences(X_train, maxlen=PAD_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=PAD_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-26 19:50:06.720 tensorflow-2-3-gpu--ml-g4dn-xlarge-794be025f5602a375b1b7feb8a0a:3143 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-06-26 19:50:06.745 tensorflow-2-3-gpu--ml-g4dn-xlarge-794be025f5602a375b1b7feb8a0a:3143 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          250000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 270,404\n",
      "Trainable params: 270,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(NUM_WORDS, EMBEDDING))\n",
    "model.add(LSTM(EMBEDDING))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint(f'../model/{MODEL_NAME}.h5', monitor='val_acc', mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "220/220 [==============================] - 5s 22ms/step - loss: 1.0940 - acc: 0.5233 - val_loss: 0.8767 - val_acc: 0.6377\n",
      "Epoch 2/30\n",
      "220/220 [==============================] - 4s 20ms/step - loss: 0.7808 - acc: 0.6982 - val_loss: 0.7550 - val_acc: 0.7180\n",
      "Epoch 3/30\n",
      "220/220 [==============================] - 4s 20ms/step - loss: 0.6487 - acc: 0.7638 - val_loss: 0.7166 - val_acc: 0.7312\n",
      "Epoch 4/30\n",
      "220/220 [==============================] - 4s 20ms/step - loss: 0.6024 - acc: 0.7775 - val_loss: 0.7002 - val_acc: 0.7375\n",
      "Epoch 5/30\n",
      "220/220 [==============================] - 4s 20ms/step - loss: 0.5614 - acc: 0.7921 - val_loss: 0.6975 - val_acc: 0.7391\n",
      "Epoch 6/30\n",
      "220/220 [==============================] - 4s 20ms/step - loss: 0.5328 - acc: 0.8019 - val_loss: 0.7015 - val_acc: 0.7385\n",
      "Epoch 7/30\n",
      "220/220 [==============================] - 4s 20ms/step - loss: 0.5162 - acc: 0.8090 - val_loss: 0.7081 - val_acc: 0.7373\n",
      "Epoch 8/30\n",
      "220/220 [==============================] - 4s 20ms/step - loss: 0.5035 - acc: 0.8127 - val_loss: 0.7522 - val_acc: 0.7327\n",
      "Epoch 9/30\n",
      "220/220 [==============================] - 5s 21ms/step - loss: 0.4921 - acc: 0.8180 - val_loss: 0.7283 - val_acc: 0.7352\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=30, callbacks=[es, mc],\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fragrantica perfume review clasifier (LSTM with stopword removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model Name\n",
    "MODEL_NAME = 'lstm_with_lemmatizated_05'\n",
    "\n",
    "# HyperParameters\n",
    "PAD_LEN = 200\n",
    "NUM_WORDS = 5000\n",
    "EMBEDDING = 200\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '../dataset/dataset_210626_215600.csv'\n",
    "data_exist = path.exists(data_path)\n",
    "\n",
    "if not data_exist:\n",
    "    url = 'https://kyuuuw-nlp-dataset.s3.ap-northeast-2.amazonaws.com/fragrantica/dataset_210626_215600.csv'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(data_path, 'w').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74779\n",
      "74779\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "\n",
    "X_data = data['lemmatizated']\n",
    "y_data = data['label']\n",
    "\n",
    "print(len(X_data))\n",
    "print(len(y_data))\n",
    "\n",
    "\n",
    "##### 토큰화 및 인덱스 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'sample', 'today', 'year', 'old', 'daughter', 'think', 'smell', 'spray', 'card', 'rotten', 'fish', 'nearly', 'throw', 'immediately', 'however', 'later', 'even', 'decide', 'give', 'fair', 'trial', 'spray', 'crook', 'elbow', 'rot', 'fish', 'smell', 'time', 'get', 'definite', 'bit', 'funk', 'almost', 'urine', 'scent', 'minute', 'blossom', 'gorgeous', 'smooth', 'woody', 'ambery', 'clean', 'warm', 'jasmine', 'daughter', 'didnt', 'even', 'believe', 'tell', 'fragrance', 'smell', 'earlier', 'lol', 'immediately', 'comment', 'fresh', 'clean', 'vibe', 'agree', 'soapy', 'hint', 'powdery', 'way', 'obsess', 'cant', 'stop', 'smelling', 'arm', 'glad', 'give', 'try', 'wait', 'buy', 'full', 'bottle']\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_data)\n",
    "sequences = tokenizer.texts_to_sequences(X_data)\n",
    "\n",
    "print(X_data[0])\n",
    "print(len(sequences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 인덱스별 단어 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 빈도수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "등장 빈도가 2번 이하인 희귀 단어의 수: 43039\n",
      "단어 집합(vocabulary)에서 희귀 단어의 비율: 0.5753877005347594\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.012692410682162147\n"
     ]
    }
   ],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(word_to_index) # 총 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도 수가 threshold 보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0\n",
    "\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    if value < threshold:\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print(f\"등장 빈도가 {threshold}번 이하인 희귀 단어의 수: {rare_cnt}\")\n",
    "print(f\"단어 집합(vocabulary)에서 희귀 단어의 비율: {rare_cnt / total_cnt}\" )\n",
    "print(f\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율: {rare_freq / total_freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 등장 횟수 상위 50000개의 word 만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 72, 170, 54, 93, 1439, 14, 1, 25, 1027, 1892, 3407, 824, 888, 463, 107, 185, 31, 309, 39, 1190, 3248, 25, 2495, 2415, 3407, 1, 15, 6, 1472, 50, 4902, 80, 2482, 3, 158, 666, 255, 320, 123, 1181, 108, 67, 152, 1439, 621, 31, 263, 191, 4, 1, 1389, 336, 463, 413, 46, 108, 175, 303, 521, 218, 168, 44, 1690, 349, 312, 895, 432, 474, 39, 28, 350, 22, 179, 11], [21, 464, 121, 2941, 964, 1, 2, 1209, 25, 2, 10, 769, 494, 64, 8, 1, 659, 2873, 61, 188, 73, 33, 64, 37, 731, 8, 163, 8, 349, 1013, 1691, 1019, 492, 10, 52, 54, 802, 11, 1080, 783, 309, 39, 28, 25, 2941, 263, 1594, 424, 213, 117, 1140, 61, 10, 152, 775, 9, 24, 357, 1, 1, 152, 1031, 67, 139, 437, 44, 24, 223, 200, 38, 13, 659, 81, 83, 1350, 2941, 78, 135, 204, 263, 73, 439, 26, 4, 2941, 30, 1, 301, 14, 40, 235, 208, 97, 90, 1611, 8, 8, 254, 223, 1595, 1629, 30, 117, 43, 14, 759, 11, 335, 7, 335, 176, 177, 601, 1101, 22, 11, 2589, 243, 15, 121, 219, 13, 823], [10, 365, 63, 238, 129, 10, 138, 52, 32, 2326, 6, 448, 6, 72, 313, 1220, 495, 85, 92, 406, 88, 135, 226, 889, 557, 388, 207, 4, 9, 152, 164, 137, 1093, 207], [314, 1407, 776, 244, 497, 21, 152, 3629, 179, 1320, 21, 41, 1, 785, 1668, 348, 276, 152, 81, 842, 51, 15, 405, 136, 118, 207, 120, 152, 201, 606, 3200, 48, 365, 27, 4633, 19, 7, 416, 2, 5, 14, 182, 825, 205, 991, 55, 1, 66, 159, 43, 184, 34, 50, 353, 160, 555, 4634, 1499, 1305, 18, 631, 21, 25, 738, 217, 27, 27, 2318, 6, 1651, 56, 189, 84, 892, 7, 402, 557, 2025, 129, 207, 9, 107, 164, 137, 209, 746, 1, 24, 2, 207, 79], [255, 255, 149, 7, 3, 265, 3, 93, 125, 40, 166, 22, 2, 74, 3, 22]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X_data)\n",
    "sequences = tokenizer.texts_to_sequences(X_data)\n",
    "\n",
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56084\n",
      "56084\n",
      "18695\n",
      "18695\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences, y_data)\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49, 10, 1, 38, 48, 20, 384, 570, 1, 48, 200, 35, 1825, 355, 127, 6, 48, 260, 1]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# padding and trimming\n",
    "X_train = pad_sequences(X_train, maxlen=PAD_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=PAD_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-26 18:28:41.241 tensorflow-2-3-gpu--ml-g4dn-xlarge-794be025f5602a375b1b7feb8a0a:682 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-06-26 18:28:41.267 tensorflow-2-3-gpu--ml-g4dn-xlarge-794be025f5602a375b1b7feb8a0a:682 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 804       \n",
      "=================================================================\n",
      "Total params: 1,321,604\n",
      "Trainable params: 1,321,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(NUM_WORDS, EMBEDDING))\n",
    "model.add(LSTM(EMBEDDING))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint(f'../model/{MODEL_NAME}.h5', monitor='val_acc', mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 0.9522 - acc: 0.6117 - val_loss: 0.7584 - val_acc: 0.7179\n",
      "Epoch 2/30\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 0.6582 - acc: 0.7564 - val_loss: 0.6871 - val_acc: 0.7415\n",
      "Epoch 3/30\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 0.5986 - acc: 0.7780 - val_loss: 0.7302 - val_acc: 0.7238\n",
      "Epoch 4/30\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 0.5637 - acc: 0.7900 - val_loss: 0.7185 - val_acc: 0.7310\n",
      "Epoch 5/30\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 0.5535 - acc: 0.7947 - val_loss: 0.7430 - val_acc: 0.7186\n",
      "Epoch 6/30\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 0.5213 - acc: 0.8063 - val_loss: 0.7952 - val_acc: 0.7070\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=30, callbacks=[es, mc],\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

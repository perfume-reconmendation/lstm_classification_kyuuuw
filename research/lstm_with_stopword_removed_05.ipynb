{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fragrantica perfume review clasifier (LSTM with stopword removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model Name\n",
    "MODEL_NAME = 'lstm_with_stopword_removed_05'\n",
    "\n",
    "# HyperParameters\n",
    "PAD_LEN = 200\n",
    "NUM_WORDS = 5000\n",
    "EMBEDDING = 200\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '../dataset/dataset_210626_215600.csv'\n",
    "data_exist = path.exists(data_path)\n",
    "\n",
    "if not data_exist:\n",
    "    url = 'https://kyuuuw-nlp-dataset.s3.ap-northeast-2.amazonaws.com/fragrantica/dataset_210626_215600.csv'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(data_path, 'w').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74779\n",
      "74779\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "\n",
    "X_data = data['stopwords_removed']\n",
    "y_data = data['label']\n",
    "\n",
    "print(len(X_data))\n",
    "print(len(y_data))\n",
    "\n",
    "\n",
    "##### 토큰화 및 인덱스 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['got', 'sample', 'today', 'year', 'old', 'daughter', 'thought', 'smelling', 'sprayed', 'card', 'rotten', 'fish', 'nearly', 'threw', 'immediately', 'however', 'later', 'evening', 'decided', 'give', 'fair', 'trial', 'sprayed', 'crook', 'elbows', 'rotting', 'fish', 'smell', 'time', 'got', 'definite', 'bit', 'funk', 'almost', 'urine', 'scent', 'minutes', 'blossomed', 'gorgeous', 'smooth', 'woody', 'ambery', 'clean', 'warm', 'jasmine', 'daughter', 'didnt', 'even', 'believe', 'told', 'fragrance', 'smelled', 'earlier', 'lol', 'immediately', 'commented', 'fresh', 'clean', 'vibe', 'agree', 'soapy', 'hint', 'powdery', 'way', 'obsessed', 'cant', 'stop', 'smelling', 'arm', 'glad', 'gave', 'try', 'wait', 'buy', 'full', 'bottle']\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_data)\n",
    "sequences = tokenizer.texts_to_sequences(X_data)\n",
    "\n",
    "print(X_data[0])\n",
    "print(len(sequences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 인덱스별 단어 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_to_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 빈도수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "등장 빈도가 2번 이하인 희귀 단어의 수: 44912\n",
      "단어 집합(vocabulary)에서 희귀 단어의 비율: 0.552307635549762\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.013189565348140857\n"
     ]
    }
   ],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(word_to_index) # 총 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도 수가 threshold 보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0\n",
    "\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    if value < threshold:\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print(f\"등장 빈도가 {threshold}번 이하인 희귀 단어의 수: {rare_cnt}\")\n",
    "print(f\"단어 집합(vocabulary)에서 희귀 단어의 비율: {rare_cnt / total_cnt}\" )\n",
    "print(f\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율: {rare_freq / total_freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 등장 횟수 상위 50000개의 word 만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41, 102, 175, 207, 142, 1698, 134, 83, 145, 1237, 2100, 4180, 874, 2645, 466, 105, 190, 474, 404, 111, 1313, 4301, 145, 3232, 4180, 5, 17, 41, 1604, 38, 77, 2819, 2, 184, 249, 355, 129, 1269, 120, 70, 155, 1698, 646, 27, 273, 587, 4, 45, 1272, 333, 466, 2065, 39, 120, 198, 330, 533, 276, 170, 36, 1699, 343, 421, 83, 575, 477, 324, 60, 532, 43, 185, 12], [15, 2041, 310, 3487, 642, 8, 1, 1432, 55, 1, 7, 806, 496, 161, 16, 8, 689, 3292, 57, 100, 67, 25, 161, 37, 801, 16, 168, 16, 343, 1263, 1095, 494, 7, 47, 90, 849, 12, 1388, 838, 404, 111, 60, 145, 3487, 273, 1762, 428, 215, 115, 1546, 57, 7, 155, 810, 35, 20, 350, 5, 5, 155, 1103, 70, 139, 442, 36, 20, 228, 572, 52, 11, 689, 459, 82, 1494, 73, 248, 632, 273, 67, 444, 138, 4, 405, 3487, 22, 8, 100, 19, 33, 244, 213, 101, 686, 2234, 16, 16, 252, 228, 1190, 1813, 22, 115, 46, 19, 1042, 12, 331, 6, 331, 187, 224, 621, 1686, 43, 12, 3753, 236, 17, 310, 268, 11, 1539], [7, 132, 58, 385, 133, 7, 132, 47, 26, 9, 454, 41, 102, 375, 1317, 510, 1185, 109, 1451, 85, 248, 234, 732, 1348, 601, 212, 4, 18, 155, 171, 416, 944, 212], [340, 985, 1954, 525, 15, 155, 185, 2531, 15, 29, 5, 823, 1783, 369, 279, 155, 459, 892, 53, 17, 1384, 154, 117, 212, 119, 155, 399, 624, 3701, 66, 132, 79, 14, 6, 1568, 1, 3, 19, 191, 875, 210, 1063, 49, 45, 76, 163, 1458, 995, 379, 38, 345, 164, 603, 2244, 647, 23, 661, 15, 55, 774, 226, 86, 86, 2720, 9, 2195, 50, 196, 88, 983, 6, 417, 1348, 133, 212, 18, 105, 171, 416, 320, 2235, 5, 20, 1, 212, 80], [249, 249, 397, 6, 2, 294, 2, 142, 181, 33, 169, 43, 1, 69, 65, 43]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X_data)\n",
    "sequences = tokenizer.texts_to_sequences(X_data)\n",
    "\n",
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56084\n",
      "56084\n",
      "18695\n",
      "18695\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences, y_data)\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[269, 72, 4, 14, 90, 811, 1192, 432, 280, 80, 9, 1012, 1222, 655, 860, 21, 18, 92, 10, 941, 47, 529, 18, 23, 867, 11, 200, 56, 23, 1041, 14, 278, 1, 655, 35, 242, 2, 628, 50, 36, 10, 974, 1, 38, 2133, 1139, 22, 23, 357, 19, 187, 787, 231, 342, 261, 342, 8, 131, 75, 278, 221, 70, 130, 1210, 183, 978, 197, 2, 30, 245, 1070, 6, 12, 963, 139, 1042]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# padding and trimming\n",
    "X_train = pad_sequences(X_train, maxlen=PAD_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=PAD_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-26 19:28:31.534 tensorflow-2-3-gpu--ml-g4dn-xlarge-794be025f5602a375b1b7feb8a0a:2390 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-06-26 19:28:31.559 tensorflow-2-3-gpu--ml-g4dn-xlarge-794be025f5602a375b1b7feb8a0a:2390 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 804       \n",
      "=================================================================\n",
      "Total params: 1,321,604\n",
      "Trainable params: 1,321,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(NUM_WORDS, EMBEDDING))\n",
    "model.add(LSTM(EMBEDDING))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint(f'../model/{MODEL_NAME}.h5', monitor='val_acc', mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 0.9607 - acc: 0.6074 - val_loss: 0.7612 - val_acc: 0.7156\n",
      "Epoch 2/30\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 0.6637 - acc: 0.7525 - val_loss: 0.6861 - val_acc: 0.7405\n",
      "Epoch 3/30\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 0.5952 - acc: 0.7775 - val_loss: 0.6996 - val_acc: 0.7364\n",
      "Epoch 4/30\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 0.5765 - acc: 0.7852 - val_loss: 0.7255 - val_acc: 0.7346\n",
      "Epoch 5/30\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 0.5466 - acc: 0.7950 - val_loss: 0.7285 - val_acc: 0.7345\n",
      "Epoch 6/30\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 0.5238 - acc: 0.8056 - val_loss: 0.7522 - val_acc: 0.7322\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=30, callbacks=[es, mc],\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
